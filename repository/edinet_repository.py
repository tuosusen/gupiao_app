"""
EDINET API ãƒªãƒã‚¸ãƒˆãƒª
é‡‘èåºã®EDINET APIã‹ã‚‰è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
"""

import requests
import pandas as pd
import zipfile
import io
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from typing import Optional, Dict, List
import re


class EDINETRepository:
    """EDINET APIã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿å–å¾—"""
    
    def __init__(self, api_key: str):
        """
        åˆæœŸåŒ–
        
        Args:
            api_key: EDINET APIã‚­ãƒ¼
        """
        self.api_key = api_key
        self.base_url = "https://api.edinet-fsa.go.jp/api/v2"
    
    def get_documents_list(self, date: str, doc_type: int = 2) -> Optional[Dict]:
        """
        æ›¸é¡ä¸€è¦§ã‚’å–å¾—
        
        Args:
            date: æ—¥ä»˜ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰
            doc_type: 1=ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿, 2=æå‡ºæ›¸é¡ä¸€è¦§åŠã³ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            æ›¸é¡ä¸€è¦§ã®è¾æ›¸ã€ã¾ãŸã¯None
        """
        url = f"{self.base_url}/documents.json"
        params = {
            'date': date,
            'type': doc_type,
            'Subscription-Key': self.api_key
        }

        try:
            response = requests.get(url, params=params, timeout=30)
            if response.status_code == 200:
                result = response.json()
                if result.get('metadata', {}).get('status') == '200':
                    return result
                else:
                    return None
            else:
                return None
        except Exception:
            return None
    
    def get_document(self, doc_id: str, doc_type: int = 1) -> Optional[bytes]:
        """
        æ›¸é¡ã‚’å–å¾—
        
        Args:
            doc_id: æ›¸é¡ID
            doc_type: 1=æå‡ºæœ¬æ–‡æ›¸åŠã³ç›£æŸ»å ±å‘Šæ›¸(XBRLå«ã‚€), 5=CSVå½¢å¼
            
        Returns:
            æ›¸é¡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆbytesï¼‰ã€ã¾ãŸã¯None
        """
        url = f"{self.base_url}/documents/{doc_id}"
        params = {
            'type': doc_type,
            'Subscription-Key': self.api_key
        }

        try:
            response = requests.get(url, params=params, timeout=60)
            if response.status_code == 200:
                return response.content
            else:
                return None
        except Exception:
            return None
    
    def extract_xbrl_data(self, zip_content: bytes) -> Optional[bytes]:
        """
        ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰XBRLãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º

        Args:
            zip_content: ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿

        Returns:
            XBRLãƒ‡ãƒ¼ã‚¿ã€ã¾ãŸã¯None
        """
        try:
            with zipfile.ZipFile(io.BytesIO(zip_content)) as zip_file:
                xbrl_files = []

                # ã™ã¹ã¦ã®XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
                for file_name in zip_file.namelist():
                    if file_name.endswith('.xbrl'):
                        xbrl_files.append(file_name)

                if not xbrl_files:
                    return None

                # è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€å¯èƒ½æ€§ãŒé«˜ã„XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆ
                # 1. PublicDocå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆè²¡å‹™è«¸è¡¨ãƒ‡ãƒ¼ã‚¿ï¼‰
                # 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„ã‚‚ã®
                priority_files = []
                for xbrl_file in xbrl_files:
                    # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã€ç›£æŸ»å ±å‘Šæ›¸ã‚’é™¤å¤–
                    if 'header' in xbrl_file.lower() or 'audit' in xbrl_file.lower():
                        continue
                    # PublicDocå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å„ªå…ˆ
                    if 'PublicDoc' in xbrl_file or 'public' in xbrl_file.lower():
                        priority_files.insert(0, xbrl_file)
                    else:
                        priority_files.append(xbrl_file)

                # å„ªå…ˆé †ä½ã®é«˜ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è©¦ã™
                target_files = priority_files if priority_files else xbrl_files

                # ã‚µã‚¤ã‚ºé †ã«ã‚½ãƒ¼ãƒˆï¼ˆå¤§ãã„ãƒ•ã‚¡ã‚¤ãƒ«ã»ã©è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€å¯èƒ½æ€§ãŒé«˜ã„ï¼‰
                file_sizes = []
                for file_name in target_files:
                    file_info = zip_file.getinfo(file_name)
                    file_sizes.append((file_name, file_info.file_size))

                file_sizes.sort(key=lambda x: x[1], reverse=True)

                # æœ€å¤§ã®XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã™
                if file_sizes:
                    largest_file = file_sizes[0][0]
                    print(f"        ğŸ’¡ {len(xbrl_files)}å€‹ã®XBRLãƒ•ã‚¡ã‚¤ãƒ«ä¸­ã€æœ€å¤§ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ: {largest_file} ({file_sizes[0][1]} bytes)")
                    with zip_file.open(largest_file) as xbrl_file:
                        return xbrl_file.read()

                return None
        except Exception as e:
            print(f"XBRLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def parse_xbrl_to_dataframe(self, xbrl_content: bytes) -> Optional[Dict[str, pd.DataFrame]]:
        """
        XBRLãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’DataFrameã«å¤‰æ›

        Args:
            xbrl_content: XBRLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿

        Returns:
            {ã‚«ãƒ†ã‚´ãƒªå: DataFrame} ã®è¾æ›¸ã€ã¾ãŸã¯None
        """
        try:
            # XMLã‚’ãƒ‘ãƒ¼ã‚¹
            root = ET.fromstring(xbrl_content)

            # è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            financial_data = {
                'æç›Šè¨ˆç®—æ›¸': [],
                'è²¸å€Ÿå¯¾ç…§è¡¨': [],
                'åŒ…æ‹¬åˆ©ç›Šè¨ˆç®—æ›¸': [],
                'ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼è¨ˆç®—æ›¸': []
            }

            # ãƒ‡ãƒãƒƒã‚°: æœ€åˆã®50å€‹ã®ã‚¿ã‚°ã‚’ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
            tag_samples = []
            elem_count = 0

            # XBRLã®è¦ç´ ã‚’æ¢ç´¢
            for elem in root.iter():
                tag_name = elem.tag
                # åå‰ç©ºé–“ã‚’é™¤å»ã—ã¦ã‚¿ã‚°åã‚’å–å¾—
                if '}' in tag_name:
                    tag_name = tag_name.split('}')[1]

                # ãƒ‡ãƒãƒƒã‚°ç”¨ã‚µãƒ³ãƒ—ãƒ«åé›†ï¼ˆæœ€åˆã®50å€‹ã®ã¿ï¼‰
                if elem_count < 50 and elem.text and elem.text.strip():
                    tag_samples.append(f"{tag_name}: {elem.text.strip()[:50]}")
                    elem_count += 1

                # è²¡å‹™æŒ‡æ¨™ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
                value = elem.text
                if value and value.strip():
                    # å£²ä¸Šé«˜é–¢é€£
                    if re.search(r'(å£²ä¸Šé«˜|NetSales|Revenue)', tag_name, re.IGNORECASE):
                        financial_data['æç›Šè¨ˆç®—æ›¸'].append({
                            'é …ç›®': 'å£²ä¸Šé«˜',
                            'ã‚¿ã‚°': tag_name,
                            'å€¤': value,
                            'ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ': elem.get('contextRef', ''),
                            'å˜ä½': elem.get('unitRef', '')
                        })

                    # å–¶æ¥­åˆ©ç›Šé–¢é€£
                    elif re.search(r'(å–¶æ¥­åˆ©ç›Š|OperatingIncome|OperatingProfit)', tag_name, re.IGNORECASE):
                        financial_data['æç›Šè¨ˆç®—æ›¸'].append({
                            'é …ç›®': 'å–¶æ¥­åˆ©ç›Š',
                            'ã‚¿ã‚°': tag_name,
                            'å€¤': value,
                            'ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ': elem.get('contextRef', ''),
                            'å˜ä½': elem.get('unitRef', '')
                        })

                    # å½“æœŸç´”åˆ©ç›Šé–¢é€£
                    elif re.search(r'(å½“æœŸç´”åˆ©ç›Š|NetIncome|ProfitLoss)', tag_name, re.IGNORECASE):
                        financial_data['æç›Šè¨ˆç®—æ›¸'].append({
                            'é …ç›®': 'å½“æœŸç´”åˆ©ç›Š',
                            'ã‚¿ã‚°': tag_name,
                            'å€¤': value,
                            'ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ': elem.get('contextRef', ''),
                            'å˜ä½': elem.get('unitRef', '')
                        })

                    # ç·è³‡ç”£é–¢é€£
                    elif re.search(r'(ç·è³‡ç”£|TotalAssets|Assets)', tag_name, re.IGNORECASE):
                        financial_data['è²¸å€Ÿå¯¾ç…§è¡¨'].append({
                            'é …ç›®': 'ç·è³‡ç”£',
                            'ã‚¿ã‚°': tag_name,
                            'å€¤': value,
                            'ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ': elem.get('contextRef', ''),
                            'å˜ä½': elem.get('unitRef', '')
                        })

                    # ç´”è³‡ç”£é–¢é€£
                    elif re.search(r'(ç´”è³‡ç”£|NetAssets|Equity)', tag_name, re.IGNORECASE):
                        financial_data['è²¸å€Ÿå¯¾ç…§è¡¨'].append({
                            'é …ç›®': 'ç´”è³‡ç”£',
                            'ã‚¿ã‚°': tag_name,
                            'å€¤': value,
                            'ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ': elem.get('contextRef', ''),
                            'å˜ä½': elem.get('unitRef', '')
                        })

                    # å–¶æ¥­ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼é–¢é€£
                    elif re.search(r'(å–¶æ¥­.*ã‚­ãƒ£ãƒƒã‚·ãƒ¥|OperatingCashFlow|CashFlowsFromOperating)', tag_name, re.IGNORECASE):
                        financial_data['ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼è¨ˆç®—æ›¸'].append({
                            'é …ç›®': 'å–¶æ¥­æ´»å‹•ã«ã‚ˆã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼',
                            'ã‚¿ã‚°': tag_name,
                            'å€¤': value,
                            'ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ': elem.get('contextRef', ''),
                            'å˜ä½': elem.get('unitRef', '')
                        })

            # ãƒ‡ãƒãƒƒã‚°: ã‚µãƒ³ãƒ—ãƒ«ã‚¿ã‚°ã‚’è¡¨ç¤º
            if tag_samples:
                print(f"        ğŸ“‹ XBRLã‚¿ã‚°ã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®{len(tag_samples)}å€‹ï¼‰:")
                for i, sample in enumerate(tag_samples[:10], 1):
                    print(f"          {i}. {sample}")

            # DataFrameã«å¤‰æ›
            result = {}
            for category, items in financial_data.items():
                if items:
                    df = pd.DataFrame(items)
                    result[category] = df

            # ãƒ‡ãƒãƒƒã‚°: ãƒãƒƒãƒã—ãŸã‚¿ã‚°ãŒãªã„å ´åˆã®è¿½åŠ æƒ…å ±
            if not result:
                print(f"        âš ï¸ è²¡å‹™æŒ‡æ¨™ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã™ã‚‹ã‚¿ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                print(f"        ğŸ’¡ XBRLãƒ•ã‚¡ã‚¤ãƒ«ã« {elem_count} å€‹ã®è¦ç´ ãŒã‚ã‚Šã¾ã™ãŒã€ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒãƒƒãƒã—ã¾ã›ã‚“ã§ã—ãŸ")

            return result if result else None

        except Exception as e:
            print(f"XBRLè§£æã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def extract_csv_data(self, zip_content: bytes) -> Optional[Dict[str, pd.DataFrame]]:
        """
        ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰CSVãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        
        Args:
            zip_content: ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿
            
        Returns:
            {ãƒ•ã‚¡ã‚¤ãƒ«å: DataFrame} ã®è¾æ›¸ã€ã¾ãŸã¯None
        """
        try:
            with zipfile.ZipFile(io.BytesIO(zip_content)) as zip_file:
                csv_data = {}
                for file_name in zip_file.namelist():
                    if file_name.endswith('.csv'):
                        with zip_file.open(file_name) as csv_file:
                            df = pd.read_csv(csv_file, encoding='cp932')
                            csv_data[file_name] = df
                return csv_data
        except Exception:
            return None
    
    def get_financial_statements(self, company_code: str, years: int = 5,
                                 doc_types: List[str] = None) -> Dict[str, Dict[str, pd.DataFrame]]:
        """
        ä¼æ¥­ã®è²¡å‹™è«¸è¡¨ã‚’å–å¾—

        Args:
            company_code: ä¼æ¥­ã‚³ãƒ¼ãƒ‰ï¼ˆè¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã¾ãŸã¯EDINETã‚³ãƒ¼ãƒ‰ï¼‰
            years: å–å¾—ã™ã‚‹å¹´æ•°
            doc_types: æ›¸é¡ç¨®é¡ã‚³ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆï¼ˆä¾‹: ['120', '140']ï¼‰

        Returns:
            {æœŸé–“: {ãƒ•ã‚¡ã‚¤ãƒ«å: DataFrame}} ã®è¾æ›¸
        """
        # doc_typesãŒNoneã®å ´åˆã¯ã™ã¹ã¦ã®æ›¸é¡ç¨®é¡ã‚’è¨±å¯ï¼ˆãƒªã‚¹ãƒˆã¨ã—ã¦æ¸¡ã•ã‚Œãªã„ï¼‰
        # ç©ºãƒªã‚¹ãƒˆã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
        if doc_types is not None and len(doc_types) == 0:
            doc_types = ['120', '140']  # æœ‰ä¾¡è¨¼åˆ¸å ±å‘Šæ›¸ã€å››åŠæœŸå ±å‘Šæ›¸

        financial_data = {}
        company_code = company_code.replace('.T', '').replace(' ', '')

        # å„å¹´ã«ã¤ã„ã¦ã€éå»Næ—¥åˆ†ã®æ›¸é¡ã‚’æ¤œç´¢
        # æ¯æ—¥ãƒã‚§ãƒƒã‚¯ï¼ˆæ›¸é¡æå‡ºæ—¥ã‚’ç¢ºå®Ÿã«ã‚«ãƒãƒ¼ï¼‰
        # æ³¨: APIåˆ¶é™ã‚’è€ƒæ…®ã—ã¦æœ€å¤§180æ—¥é–“ã«åˆ¶é™
        end_date = datetime.now()
        max_days = min(365 * years, 180)  # æœ€å¤§180æ—¥é–“
        start_date = end_date - timedelta(days=max_days)

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ç”¨ã®å¤‰æ•°
        total_checked_dates = 0
        dates_with_docs = 0
        matching_docs_count = 0
        sample_sec_codes = []  # ã‚µãƒ³ãƒ—ãƒ«è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚’åé›†

        # æ¯æ—¥ãƒã‚§ãƒƒã‚¯ï¼ˆæ›¸é¡æå‡ºæ—¥ã‚’ç¢ºå®Ÿã«ã‚«ãƒãƒ¼ï¼‰
        current_date = end_date
        while current_date >= start_date:
            date_str = current_date.strftime('%Y-%m-%d')
            total_checked_dates += 1

            documents = self.get_documents_list(date_str)
            if not documents:
                current_date -= timedelta(days=1)
                continue

            dates_with_docs += 1

            # å¯¾è±¡ä¼æ¥­ã®æ›¸é¡ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            company_docs = []
            for idx, doc in enumerate(documents.get('results', [])):
                sec_code = (doc.get('secCode') or '').replace(' ', '')
                edinet_code = doc.get('edinetCode') or ''
                filer_name = doc.get('filerName', '')

                # ã‚µãƒ³ãƒ—ãƒ«è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã‚’åé›†ï¼ˆæœ€åˆã®10ä»¶ã®ã¿ï¼‰
                if len(sample_sec_codes) < 10 and sec_code:
                    sample_sec_codes.append(f"{filer_name[:20]}... | {sec_code}")

                # ãƒ‡ãƒãƒƒã‚°: ä¼æ¥­ã‚³ãƒ¼ãƒ‰ãŒä¸€è‡´ã™ã‚‹æ›¸é¡ã‚’è©³ç´°ãƒ­ã‚°å‡ºåŠ›
                if company_code in sec_code or (filer_name and company_code in filer_name):
                    doc_type_for_debug = doc.get('docTypeCode')
                    print(f"  å€™è£œç™ºè¦‹: {filer_name} | è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰: '{sec_code}' | EDINETã‚³ãƒ¼ãƒ‰: {edinet_code} | æ›¸é¡ç¨®é¡: {doc_type_for_debug}")

                if (sec_code.startswith(company_code) or edinet_code == company_code):
                    doc_type = doc.get('docTypeCode')
                    print(f"    â†’ ä¼æ¥­ã‚³ãƒ¼ãƒ‰ä¸€è‡´ãƒã‚§ãƒƒã‚¯: sec_code='{sec_code}', doc_type='{doc_type}', æœŸå¾…ã•ã‚Œã‚‹æ›¸é¡ç¨®é¡={doc_types}")
                    # doc_typesãŒNoneã®å ´åˆã¯ã™ã¹ã¦ã®æ›¸é¡ã‚’å—ã‘å…¥ã‚Œã‚‹
                    if doc_types is None or doc_type in doc_types:
                        company_docs.append(doc)
                        matching_docs_count += 1
                        print(f"    âœ“ ãƒãƒƒãƒæˆåŠŸ: {filer_name} | æ›¸é¡ç¨®é¡: {doc_type}")
                    else:
                        print(f"    âœ— æ›¸é¡ç¨®é¡ä¸ä¸€è‡´: doc_type='{doc_type}' not in {doc_types}")

            # å„æ›¸é¡ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            for doc in company_docs:
                doc_id = doc.get('docID')
                doc_type_code = doc.get('docTypeCode')
                filer_name_for_dl = doc.get('filerName', '')

                print(f"      â†’ æ›¸é¡ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è©¦è¡Œ: {doc_id} | ç¨®é¡: {doc_type_code}")
                doc_content = self.get_document(doc_id, doc_type=1)  # XBRLå½¢å¼ã«å¤‰æ›´

                if doc_content:
                    print(f"        âœ“ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ ({len(doc_content)} bytes)")

                    # XBRLãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                    xbrl_content = self.extract_xbrl_data(doc_content)
                    if xbrl_content:
                        print(f"        âœ“ XBRLæŠ½å‡ºæˆåŠŸ ({len(xbrl_content)} bytes)")

                        # XBRLã‚’ãƒ‘ãƒ¼ã‚¹ã—ã¦DataFrameã«å¤‰æ›
                        parsed_data = self.parse_xbrl_to_dataframe(xbrl_content)
                        if parsed_data:
                            period = doc.get('periodEnd', 'Unknown')
                            financial_data[period] = parsed_data
                            print(f"        âœ“ XBRLè§£ææˆåŠŸ: {len(parsed_data)} ã‚«ãƒ†ã‚´ãƒª")
                            for category, df in parsed_data.items():
                                print(f"          - {category}: {len(df)} é …ç›®")
                        else:
                            print(f"        âœ— XBRLè§£æå¤±æ•—: è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
                    else:
                        print(f"        âœ— XBRLæŠ½å‡ºå¤±æ•—: ZIPã«XBRLãƒ•ã‚¡ã‚¤ãƒ«ãªã—")
                else:
                    print(f"        âœ— ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ãªã—")

            current_date -= timedelta(days=1)

        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å«ã‚ã¦è¿”ã™ï¼ˆä¸€æ™‚çš„ï¼‰
        print(f"\n===== ãƒ‡ãƒãƒƒã‚°æƒ…å ± =====")
        print(f"æ¤œç´¢å¯¾è±¡ä¼æ¥­ã‚³ãƒ¼ãƒ‰: '{company_code}'")
        print(f"ãƒã‚§ãƒƒã‚¯ã—ãŸæ—¥ä»˜æ•°: {total_checked_dates}")
        print(f"æ›¸é¡ãŒã‚ã£ãŸæ—¥ä»˜æ•°: {dates_with_docs}")
        print(f"ãƒãƒƒãƒã—ãŸæ›¸é¡æ•°: {matching_docs_count}")
        if sample_sec_codes:
            print(f"\nã‚µãƒ³ãƒ—ãƒ«è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ï¼ˆæœ€åˆã®10ä»¶ï¼‰:")
            for sample in sample_sec_codes[:10]:
                print(f"  {sample}")
        print(f"====================\n")

        return financial_data
